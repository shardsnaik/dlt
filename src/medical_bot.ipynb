{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c87bbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException, UploadFile, File\n",
    "from fastapi.responses import JSONResponse\n",
    "from dotenv import load_dotenv\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "import os\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "490966fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "\n",
    "class MedicalBot:\n",
    "    def __init__(self):\n",
    "        self.index = None\n",
    "        self.model_path = \"C:\\\\Users\\\\Public\\\\Rag_based_chatbot\\\\Medical_bot\\\\Models\\\\medgemma-4b-it-medical-gguf\\\\medgemma-4b-it-finnetunned-merged_new_for_cpu_q5_k_m.gguf\"\n",
    "        self.num_threads = os.cpu_count()\n",
    "        self.conversational_memory = {}\n",
    "        \n",
    "        self.executor = ThreadPoolExecutor(max_workers=4)\n",
    "\n",
    "        self.initialize_services()\n",
    "    \n",
    "    def initialize_services(self):\n",
    "        self.llm = LlamaCpp(\n",
    "            model_path=self.model_path,\n",
    "            n_ctx=100000,       # Context size\n",
    "            n_threads=self.num_threads,\n",
    "            n_batch=512,      # Increases throughput\n",
    "            verbose=False,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            max_tokens=256,\n",
    "            stop=[\"<end_of_turn>\"])\n",
    "        \n",
    "        self.memory = ConversationBufferWindowMemory(\n",
    "             k = 5,\n",
    "             return_messages=True,\n",
    "             memory_key='chat_history'\n",
    "        )\n",
    "\n",
    "        self.prompt_template = PromptTemplate(\n",
    "            input_variables=[\"chat_history\", \"input\"],\n",
    "            template=\"\"\"You are a helpful medical assistant. Use the conversation history to provide contextual responses.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "\n",
    "<start_of_turn>user\n",
    "{input}<end_of_turn>\n",
    "<start_of_turn>assistant\n",
    "\"\"\"\n",
    "        )\n",
    "\n",
    "        self.conversation_chain = ConversationChain(\n",
    "            llm=self.llm,\n",
    "            memory=self.memory,\n",
    "            prompt=self.prompt_template,\n",
    "            verbose=True,\n",
    "            output_parser=MedicalChatBotOutputparser()\n",
    "        )\n",
    "\n",
    "    def chat(self, user_input:str)-> str:\n",
    "        try:\n",
    "            response = self.conversation_chain.predict(input=user_input)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            return f\"Sorry, I encountered an error: {str(e)}\"\n",
    "    \n",
    "\n",
    "class MedicalChatBotOutputparser:\n",
    "\n",
    "    def parse(self, text: str)->str:\n",
    "            cleaned_text = text.strip()\n",
    "            return cleaned_text\n",
    "    \n",
    "    @property\n",
    "    def _type(self)-> str:\n",
    "         return 'medgemma_output_parser'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bce7cd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [912]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:65134 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:65134 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
      "INFO:     127.0.0.1:65135 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:65135 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:65144 - \"POST /chat HTTP/1.1\" 500 Internal Server Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 403, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\starlette\\applications.py\", line 113, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 187, in __call__\n",
      "    raise exc\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 165, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\starlette\\middleware\\cors.py\", line 93, in __call__\n",
      "    await self.simple_response(scope, receive, send, request_headers=headers)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\starlette\\middleware\\cors.py\", line 144, in simple_response\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\starlette\\routing.py\", line 715, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\starlette\\routing.py\", line 735, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\starlette\\routing.py\", line 288, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\starlette\\routing.py\", line 76, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\starlette\\routing.py\", line 73, in app\n",
      "    response = await f(request)\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\fastapi\\routing.py\", line 301, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\fastapi\\routing.py\", line 212, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_912\\322237988.py\", line 22, in chat_page\n",
      "    med = MedicalBot()\n",
      "          ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_912\\3679234692.py\", line 13, in __init__\n",
      "    self.initialize_services()\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_912\\3679234692.py\", line 16, in initialize_services\n",
      "    self.llm = LlamaCpp(\n",
      "               ^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\load\\serializable.py\", line 125, in __init__\n",
      "    super().__init__(*args, **kwargs)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pydantic\\main.py\", line 253, in __init__\n",
      "    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "pydantic_core._pydantic_core.ValidationError: 1 validation error for LlamaCpp\n",
      "  Value error, Could not load Llama model from path: C:\\Users\\Public\\Rag_based_chatbot\\Medical_bot\\Models\\medgemma-4b-it-medical-gguf\\medgemma-4b-it-finnetunned-merged_new_for_cpu_q5_k_m.gguf. Received error exception: access violation reading 0x0000000000000000 [type=value_error, input_value={'model_path': 'C:\\\\Users...: None, 'grammar': None}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n",
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [912]\n"
     ]
    }
   ],
   "source": [
    "app = FastAPI(title='Medical_bot')\n",
    "app.add_middleware(\n",
    "    CORSMiddleware, \n",
    "    allow_origins =[\n",
    "        'http://loacalhost:3000'\n",
    "    ],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"GET\", \"POST\", \"OPTIONS\"],  # Include OPTIONS for preflight\n",
    "    allow_headers=[\"Content-Type\", \"Authorization\"],  # List expected headers\n",
    ")\n",
    "\n",
    "class QueryHandler(BaseModel):\n",
    "    query: str\n",
    "\n",
    "@app.get('/')\n",
    "async def home_page():\n",
    "    return {'message':\" Application is running\"}\n",
    "\n",
    "@app.post('/chat')\n",
    "async def chat_page(req: QueryHandler):\n",
    "    query = req.query\n",
    "    med = MedicalBot()\n",
    "    res = await med.chat(query)\n",
    "    return res\n",
    "\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "if __name__ =='__main__':\n",
    "    uvicorn.run(app, host='127.0.0.1', port=8000)\n",
    "# it is a classic issue when you're trying to run uvicorn.run() inside an environment that already has an active event loopâ€”like Jupyter Notebook, IPython, or certain IDEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c49bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
